#!/usr/bin/env python3
"""
F1 äº‹æ•…åš´é‡ç¨‹åº¦åˆ†ä½ˆæ¨¡çµ„ - Function 7
Accident Severity Distribution Module - Following Core Development Standards
åˆ†æäº‹æ•…çš„åš´é‡ç¨‹åº¦åˆ†ä½ˆï¼ŒåŒ…å«é¢¨éšªè©•ä¼°å’Œå®‰å…¨æ€§åˆ†æ
"""

import os
import json
import pickle
import pandas as pd
from datetime import datetime
from prettytable import PrettyTable

def generate_cache_key(session_info):
    """ç”Ÿæˆå¿«å–éµå€¼"""
    return f"severity_distribution_{session_info.get('year', 2024)}_{session_info.get('event_name', 'Unknown')}_{session_info.get('session_type', 'R')}"

def check_cache(cache_key):
    """æª¢æŸ¥å¿«å–æ˜¯å¦å­˜åœ¨"""
    cache_path = os.path.join("cache", f"{cache_key}.pkl")
    if os.path.exists(cache_path):
        try:
            with open(cache_path, 'rb') as f:
                return pickle.load(f)
        except:
            return None
    return None

def save_cache(data, cache_key):
    """ä¿å­˜æ•¸æ“šåˆ°å¿«å–"""
    os.makedirs("cache", exist_ok=True)
    cache_path = os.path.join("cache", f"{cache_key}.pkl")
    try:
        with open(cache_path, 'wb') as f:
            pickle.dump(data, f)
        return True
    except:
        return False

def assess_severity(message):
    """è©•ä¼°äº‹æ•…åš´é‡ç¨‹åº¦"""
    message = message.upper()
    
    # CRITICAL - æœ€é«˜åš´é‡ç¨‹åº¦
    if any(word in message for word in ['RED FLAG', 'RACE SUSPENSION', 'MEDICAL CAR', 'AMBULANCE', 'CHEQUERED FLAG']):
        return 'CRITICAL'
    
    # HIGH - é«˜åš´é‡ç¨‹åº¦  
    elif any(word in message for word in ['SAFETY CAR', 'YELLOW FLAG', 'COLLISION', 'CRASH']):
        return 'HIGH'
    
    # MEDIUM - ä¸­ç­‰åš´é‡ç¨‹åº¦
    elif any(word in message for word in ['INVESTIGATION', 'CONTACT', 'PENALTY', 'TIME PENALTY']):
        return 'MEDIUM'
    
    # LOW - ä½åš´é‡ç¨‹åº¦
    else:
        return 'LOW'

def calculate_risk_score(severity_data):
    """è¨ˆç®—æ¯”è³½é¢¨éšªè©•åˆ† (0-100)"""
    weights = {
        'CRITICAL': 25,
        'HIGH': 15,
        'MEDIUM': 8,
        'LOW': 3
    }
    
    total_score = 0
    for severity, count in severity_data.items():
        total_score += count * weights.get(severity, 0)
    
    # æ¨™æº–åŒ–åˆ° 0-100 åˆ†
    max_possible = 100  # å‡è¨­æœ€é«˜é¢¨éšªæƒ…æ³
    risk_score = min(total_score, max_possible)
    
    return risk_score

def get_safety_level(risk_score):
    """æ ¹æ“šé¢¨éšªè©•åˆ†ç²å–å®‰å…¨ç­‰ç´š"""
    if risk_score >= 80:
        return "æ¥µé«˜é¢¨éšª", "ğŸ”´"
    elif risk_score >= 60:
        return "é«˜é¢¨éšª", "ğŸŸ "
    elif risk_score >= 40:
        return "ä¸­ç­‰é¢¨éšª", "ğŸŸ¡"
    elif risk_score >= 20:
        return "ä½é¢¨éšª", "ğŸŸ¢"
    else:
        return "å®‰å…¨", "âœ…"

def analyze_severity_distribution(session):
    """åˆ†æäº‹æ•…åš´é‡ç¨‹åº¦åˆ†ä½ˆ"""
    severity_data = {
        'severity_distribution': {
            'CRITICAL': 0,
            'HIGH': 0,
            'MEDIUM': 0,
            'LOW': 0
        },
        'severity_details': [],
        'risk_assessment': {
            'overall_risk_score': 0,
            'safety_level': 'UNKNOWN',
            'safety_icon': 'â“',
            'most_common_severity': 'NONE',
            'critical_incidents_count': 0,
            'safety_recommendations': []
        },
        'incident_progression': [],
        'severity_by_lap': {}
    }
    
    try:
        if hasattr(session, 'race_control_messages'):
            race_control = session.race_control_messages
            
            if race_control is not None and not race_control.empty:
                # æ‰¾å‡ºæœ€å¾Œä¸€åœˆåœˆæ•¸ï¼Œç”¨æ–¼éæ¿¾æ­£å¸¸çµæŸçš„ CHEQUERED FLAG
                max_lap = race_control['Lap'].max() if 'Lap' in race_control.columns else 0
                incidents_by_lap = {}
                
                for _, message in race_control.iterrows():
                    msg_text = str(message.get('Message', '')).upper()
                    lap = message.get('Lap', 0)
                    time = message.get('Time', 'N/A')
                    
                    # éæ¿¾æœ€å¾Œä¸€åœˆçš„æ­£å¸¸æ¯”è³½çµæŸ CHEQUERED FLAG
                    if 'CHEQUERED FLAG' in msg_text and lap == max_lap:
                        continue  # è·³éæ­£å¸¸çš„æ¯”è³½çµæŸæ¨™èªŒ
                    
                    # è­˜åˆ¥äº‹æ•…ç›¸é—œé—œéµå­—
                    if any(keyword in msg_text for keyword in [
                        'ACCIDENT', 'COLLISION', 'CRASH', 'INCIDENT', 
                        'SAFETY CAR', 'RED FLAG', 'YELLOW FLAG',
                        'INVESTIGATION', 'PENALTY', 'CONTACT'
                    ]):
                        severity = assess_severity(msg_text)
                        severity_data['severity_distribution'][severity] += 1
                        
                        # è©³ç´°è¨˜éŒ„
                        incident_detail = {
                            'lap': lap,
                            'time': str(time),
                            'severity': severity,
                            'message': message.get('Message', ''),
                            'category': categorize_incident(msg_text)
                        }
                        severity_data['severity_details'].append(incident_detail)
                        
                        # æŒ‰åœˆæ•¸è¨˜éŒ„åš´é‡ç¨‹åº¦
                        if lap not in incidents_by_lap:
                            incidents_by_lap[lap] = []
                        incidents_by_lap[lap].append(severity)
                
                severity_data['severity_by_lap'] = incidents_by_lap
                
                # è¨ˆç®—é¢¨éšªè©•ä¼°
                risk_score = calculate_risk_score(severity_data['severity_distribution'])
                safety_level, safety_icon = get_safety_level(risk_score)
                
                # æ‰¾å‡ºæœ€å¸¸è¦‹çš„åš´é‡ç¨‹åº¦
                most_common = max(severity_data['severity_distribution'], 
                                key=severity_data['severity_distribution'].get)
                if severity_data['severity_distribution'][most_common] == 0:
                    most_common = 'NONE'
                
                # ç”Ÿæˆå®‰å…¨å»ºè­°
                recommendations = generate_safety_recommendations(severity_data['severity_distribution'])
                
                severity_data['risk_assessment'] = {
                    'overall_risk_score': risk_score,
                    'safety_level': safety_level,
                    'safety_icon': safety_icon,
                    'most_common_severity': most_common,
                    'critical_incidents_count': severity_data['severity_distribution']['CRITICAL'],
                    'safety_recommendations': recommendations
                }
                
                # äº‹æ•…å‡ç´šè¶¨å‹¢
                progression = analyze_incident_progression(severity_data['severity_details'])
                severity_data['incident_progression'] = progression
        
        return severity_data
        
    except Exception as e:
        print(f"[ERROR] åˆ†æåš´é‡ç¨‹åº¦åˆ†ä½ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return severity_data

def categorize_incident(message):
    """äº‹æ•…åˆ†é¡"""
    message = message.upper()
    if any(word in message for word in ['ACCIDENT', 'COLLISION', 'CRASH', 'CONTACT']):
        return 'ACCIDENT'
    elif any(word in message for word in ['SAFETY CAR', 'RED FLAG', 'YELLOW FLAG', 'CHEQUERED FLAG']):
        return 'FLAG'
    elif 'INVESTIGATION' in message:
        return 'INVESTIGATION'
    elif 'PENALTY' in message:
        return 'PENALTY'
    else:
        return 'OTHER'

def generate_safety_recommendations(severity_dist):
    """ç”Ÿæˆå®‰å…¨å»ºè­°"""
    recommendations = []
    
    if severity_dist['CRITICAL'] > 0:
        recommendations.append("å»ºè­°åŠ å¼·è³½é“å®‰å…¨æªæ–½")
        recommendations.append("æª¢è¨ç·Šæ€¥æ‡‰è®Šç¨‹åº")
    
    if severity_dist['HIGH'] > 2:
        recommendations.append("å»ºè­°å¢åŠ å®‰å…¨è»Šå¾…å‘½æ™‚é–“")
        recommendations.append("åŠ å¼·è»Šæ‰‹å®‰å…¨åŸ¹è¨“")
    
    if severity_dist['MEDIUM'] > 5:
        recommendations.append("æª¢è¨è³½è»Šè¦å‰‡åŸ·è¡Œ")
        recommendations.append("åŠ å¼·è»Šæ‰‹è³½é“è¡Œç‚ºç›£ç®¡")
    
    if sum(severity_dist.values()) == 0:
        recommendations.append("æ¯”è³½å®‰å…¨ç‹€æ³è‰¯å¥½")
        recommendations.append("ä¿æŒç¾æœ‰å®‰å…¨æ¨™æº–")
    
    return recommendations

def analyze_incident_progression(incidents):
    """åˆ†æäº‹æ•…å‡ç´šè¶¨å‹¢"""
    progression = []
    
    # æŒ‰æ™‚é–“æ’åº
    sorted_incidents = sorted(incidents, key=lambda x: x['lap'])
    
    severity_values = {'LOW': 1, 'MEDIUM': 2, 'HIGH': 3, 'CRITICAL': 4}
    
    for i, incident in enumerate(sorted_incidents):
        severity_value = severity_values.get(incident['severity'], 0)
        progression.append({
            'sequence': i + 1,
            'lap': incident['lap'],
            'severity': incident['severity'],
            'severity_value': severity_value,
            'trend': 'ESCALATING' if i > 0 and severity_value > severity_values.get(sorted_incidents[i-1]['severity'], 0) else 'STABLE'
        })
    
    return progression

def display_severity_distribution(data):
    """é¡¯ç¤ºåš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æè¡¨æ ¼"""
    print(f"\nâš ï¸ äº‹æ•…åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ (Function 7):")
    
    total_incidents = sum(data['severity_distribution'].values())
    
    if total_incidents == 0:
        print("âœ… æœ¬å ´æ¯”è³½æœªç™¼ç¾ä»»ä½•äº‹æ•…è¨˜éŒ„ï¼Œå®‰å…¨ç‹€æ³å„ªè‰¯ï¼")
        return
    
    # é¢¨éšªè©•ä¼°æ‘˜è¦
    risk_info = data['risk_assessment']
    summary_table = PrettyTable()
    summary_table.field_names = ["è©•ä¼°é …ç›®", "çµæœ", "èªªæ˜"]
    
    summary_table.add_row(["ç¸½é«”é¢¨éšªè©•åˆ†", f"{risk_info['overall_risk_score']}/100", "ç¶œåˆé¢¨éšªæŒ‡æ•¸"])
    summary_table.add_row(["å®‰å…¨ç­‰ç´š", f"{risk_info['safety_icon']} {risk_info['safety_level']}", "ç•¶å‰å®‰å…¨ç‹€æ…‹"])
    summary_table.add_row(["ä¸»è¦é¢¨éšªé¡å‹", risk_info['most_common_severity'], "æœ€å¸¸è¦‹çš„äº‹æ•…åš´é‡ç¨‹åº¦"])
    summary_table.add_row(["é—œéµäº‹ä»¶æ•¸é‡", risk_info['critical_incidents_count'], "éœ€ç‰¹åˆ¥é—œæ³¨çš„åš´é‡äº‹æ•…"])
    
    print("\nğŸ“Š é¢¨éšªè©•ä¼°æ‘˜è¦:")
    print(summary_table)
    
    # åš´é‡ç¨‹åº¦åˆ†ä½ˆè¡¨æ ¼
    severity_table = PrettyTable()
    severity_table.field_names = ["åš´é‡ç¨‹åº¦", "äº‹æ•…æ•¸é‡", "ç™¾åˆ†æ¯”", "é¢¨éšªç­‰ç´š", "èªªæ˜"]
    
    severity_descriptions = {
        'CRITICAL': ('ğŸ”´ æ¥µå±éšª', 'è³½äº‹ä¸­æ–·æˆ–ç”Ÿå‘½å®‰å…¨å¨è„…'),
        'HIGH': ('ğŸŸ  é«˜å±éšª', 'éœ€è¦å®‰å…¨è»Šæˆ–é»ƒæ——å¹²é '),
        'MEDIUM': ('ğŸŸ¡ ä¸­ç­‰é¢¨éšª', 'éœ€è¦èª¿æŸ¥æˆ–è™•ç½°çš„äº‹ä»¶'),
        'LOW': ('ğŸŸ¢ ä½é¢¨éšª', 'è¼•å¾®é•è¦æˆ–æŠ€è¡“å•é¡Œ')
    }
    
    for severity, count in data['severity_distribution'].items():
        if count > 0:
            percentage = f"{(count/total_incidents)*100:.1f}%"
            risk_level, description = severity_descriptions.get(severity, ('â“ æœªçŸ¥', 'æœªåˆ†é¡äº‹ä»¶'))
            severity_table.add_row([severity, count, percentage, risk_level, description])
    
    print("\nâš ï¸ åš´é‡ç¨‹åº¦è©³ç´°åˆ†ä½ˆ:")
    print(severity_table)
    
    # å®‰å…¨å»ºè­°è¡¨æ ¼
    if risk_info['safety_recommendations']:
        rec_table = PrettyTable()
        rec_table.field_names = ["å»ºè­°ç·¨è™Ÿ", "å®‰å…¨å»ºè­°"]
        
        for i, recommendation in enumerate(risk_info['safety_recommendations'], 1):
            rec_table.add_row([f"å»ºè­°{i}", recommendation])
        
        print(f"\nğŸ’¡ å®‰å…¨æ”¹å–„å»ºè­°:")
        print(rec_table)
    
    # äº‹æ•…å‡ç´šè¶¨å‹¢ (é¡¯ç¤ºå‰5å€‹äº‹ä»¶)
    if data['incident_progression']:
        trend_table = PrettyTable()
        trend_table.field_names = ["é †åº", "åœˆæ•¸", "åš´é‡ç¨‹åº¦", "è¶¨å‹¢", "å‚™è¨»"]
        
        for incident in data['incident_progression'][:5]:
            trend_icon = "â¬†ï¸" if incident['trend'] == 'ESCALATING' else "â¡ï¸"
            trend_table.add_row([
                incident['sequence'],
                f"ç¬¬{incident['lap']}åœˆ",
                incident['severity'],
                f"{trend_icon} {incident['trend']}",
                "éœ€é—œæ³¨" if incident['trend'] == 'ESCALATING' else "æ­£å¸¸"
            ])
        
        print(f"\nğŸ“ˆ äº‹æ•…å‡ç´šè¶¨å‹¢ (å‰5å€‹äº‹ä»¶):")
        print(trend_table)

def save_json_results(data, session_info):
    """ä¿å­˜åˆ†æçµæœç‚º JSON æª”æ¡ˆ"""
    os.makedirs("json", exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    event_name = session_info.get('event_name', 'Unknown').replace(' ', '_')
    filename = f"severity_distribution_{session_info.get('year', 2024)}_{event_name}_{timestamp}.json"
    filepath = os.path.join("json", filename)
    
    json_result = {
        "function_id": 7,
        "function_name": "Severity Distribution Analysis",
        "analysis_type": "severity_distribution",
        "session_info": session_info,
        "timestamp": datetime.now().isoformat(),
        "data": data
    }
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(json_result, f, ensure_ascii=False, indent=2)
        
        abs_filepath = os.path.abspath(filepath)
        print(f"ğŸ’¾ JSONçµæœå·²ä¿å­˜åˆ°: file:///{abs_filepath}")
        return True
    except Exception as e:
        print(f"âŒ JSONä¿å­˜å¤±æ•—: {e}")
        return False

def report_analysis_results(data, analysis_type="åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ"):
    """å ±å‘Šåˆ†æçµæœç‹€æ…‹"""
    if not data:
        print(f"âŒ {analysis_type}å¤±æ•—ï¼šç„¡å¯ç”¨æ•¸æ“š")
        return False
    
    total_incidents = sum(data.get('severity_distribution', {}).values())
    risk_score = data.get('risk_assessment', {}).get('overall_risk_score', 0)
    critical_count = data.get('severity_distribution', {}).get('CRITICAL', 0)
    
    print(f"ğŸ“Š {analysis_type}çµæœæ‘˜è¦ï¼š")
    print(f"   â€¢ ç¸½äº‹æ•…æ•¸é‡: {total_incidents}")
    print(f"   â€¢ é¢¨éšªè©•åˆ†: {risk_score}/100")
    print(f"   â€¢ é—œéµäº‹ä»¶æ•¸é‡: {critical_count}")
    print(f"   â€¢ æ•¸æ“šå®Œæ•´æ€§: {'âœ… è‰¯å¥½' if total_incidents >= 0 else 'âŒ ç•°å¸¸'}")
    
    print(f"âœ… {analysis_type}åˆ†æå®Œæˆï¼")
    return True

def run_severity_distribution_analysis(data_loader, year=None, race=None, session='R'):
    """åŸ·è¡Œåš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ - Function 7"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œåš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ...")
    
    # 1. ç²å–è³½äº‹è³‡è¨Š
    session_info = {
        "event_name": race or "Unknown",
        "circuit_name": "Unknown",
        "session_type": session,
        "year": year or 2024
    }
    
    if hasattr(data_loader, 'session') and data_loader.session is not None:
        session_info.update({
            "event_name": getattr(data_loader.session, 'event', {}).get('EventName', race or 'Unknown'),
            "circuit_name": getattr(data_loader.session, 'event', {}).get('Location', 'Unknown'),
            "session_type": getattr(data_loader.session, 'session_info', {}).get('Type', session),
            "year": getattr(data_loader.session, 'event', {}).get('year', year or 2024)
        })
    
    # 2. æª¢æŸ¥å¿«å–
    cache_key = generate_cache_key(session_info)
    cached_data = check_cache(cache_key)
    
    if cached_data:
        print("ğŸ“¦ ä½¿ç”¨ç·©å­˜æ•¸æ“š")
        severity_data = cached_data
    else:
        print("ğŸ”„ é‡æ–°è¨ˆç®— - é–‹å§‹æ•¸æ“šåˆ†æ...")
        
        # 3. åŸ·è¡Œåˆ†æ
        if hasattr(data_loader, 'session') and data_loader.session is not None:
            severity_data = analyze_severity_distribution(data_loader.session)
        else:
            print("âŒ ç„¡æ³•ç²å–è³½äº‹æ•¸æ“š")
            return None
        
        if severity_data:
            # 4. ä¿å­˜å¿«å–
            if save_cache(severity_data, cache_key):
                print("ğŸ’¾ åˆ†æçµæœå·²ç·©å­˜")
    
    # 5. çµæœé©—è­‰å’Œåé¥‹
    if not report_analysis_results(severity_data, "åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ"):
        return None
    
    # 6. é¡¯ç¤ºåˆ†æçµæœè¡¨æ ¼
    display_severity_distribution(severity_data)
    
    # 7. ä¿å­˜ JSON çµæœ
    save_json_results(severity_data, session_info)
    
    print("\nâœ… åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æå®Œæˆï¼")
    return severity_data

def run_severity_distribution_analysis_json(data_loader, dynamic_team_mapping=None, f1_analysis_instance=None, enable_debug=False, show_detailed_output=True):
    """åŸ·è¡Œåš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æä¸¦è¿”å›JSONæ ¼å¼çµæœ - APIå°ˆç”¨ç‰ˆæœ¬
    
    Args:
        data_loader: æ•¸æ“šè¼‰å…¥å™¨
        dynamic_team_mapping: å‹•æ…‹è»ŠéšŠæ˜ å°„
        f1_analysis_instance: F1åˆ†æå¯¦ä¾‹
        enable_debug: æ˜¯å¦å•Ÿç”¨èª¿è©¦æ¨¡å¼
        show_detailed_output: æ˜¯å¦é¡¯ç¤ºè©³ç´°è¼¸å‡ºï¼ˆå³ä½¿ä½¿ç”¨ç·©å­˜ä¹Ÿé¡¯ç¤ºå®Œæ•´è¡¨æ ¼ï¼‰
    """
    if enable_debug:
        print(f"\n[NEW_MODULE] åŸ·è¡Œåš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†ææ¨¡çµ„ (JSONè¼¸å‡ºç‰ˆ)...")
    
    try:
        # ç²å–è³½äº‹è³‡è¨Š
        session_info = {}
        if hasattr(data_loader, 'session') and data_loader.session is not None:
            session_info = {
                "event_name": getattr(data_loader.session, 'event', {}).get('EventName', 'Unknown'),
                "circuit_name": getattr(data_loader.session, 'event', {}).get('Location', 'Unknown'),
                "session_type": getattr(data_loader.session, 'session_info', {}).get('Type', 'Unknown'),
                "year": getattr(data_loader.session, 'event', {}).get('year', 2024)
            }
        
        # Function 15 æ¨™æº– - æª¢æŸ¥ç·©å­˜
        cache_key = generate_cache_key(session_info)
        cached_data = check_cache(cache_key)
        cache_used = cached_data is not None
        
        if cached_data and not show_detailed_output:
            if enable_debug:
                print("ğŸ“¦ ä½¿ç”¨ç·©å­˜æ•¸æ“š")
            return {
                "success": True,
                "message": "æˆåŠŸåŸ·è¡Œ åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ (ç·©å­˜)",
                "data": {
                    "function_id": 7,
                    "function_name": "Severity Distribution Analysis",
                    "analysis_type": "severity_distribution",
                    "session_info": session_info,
                    "severity_analysis": cached_data
                },
                "cache_used": True,
                "cache_key": cache_key,
                "timestamp": datetime.now().isoformat()
            }
        elif cached_data and show_detailed_output:
            if enable_debug:
                print("ğŸ“¦ ä½¿ç”¨ç·©å­˜æ•¸æ“š + ğŸ“Š é¡¯ç¤ºè©³ç´°åˆ†æçµæœ")
            
            # é¡¯ç¤ºè©³ç´°è¼¸å‡º - å³ä½¿ä½¿ç”¨ç·©å­˜ä¹Ÿé¡¯ç¤ºå®Œæ•´è¡¨æ ¼
            _display_cached_detailed_output(cached_data, session_info)
            
            return {
                "success": True,
                "message": "æˆåŠŸåŸ·è¡Œ åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ (ç·©å­˜+è©³ç´°)",
                "data": {
                    "function_id": 7,
                    "function_name": "Severity Distribution Analysis",
                    "analysis_type": "severity_distribution",
                    "session_info": session_info,
                    "severity_analysis": cached_data
                },
                "cache_used": True,
                "cache_key": cache_key,
                "timestamp": datetime.now().isoformat()
            }
        else:
            if enable_debug:
                print("ğŸ”„ é‡æ–°è¨ˆç®— - é–‹å§‹æ•¸æ“šåˆ†æ...")
        
        # åŸ·è¡Œåˆ†æ
        severity_data = run_severity_distribution_analysis(
            data_loader, 
            year=session_info.get('year'),
            race=session_info.get('event_name'),
            session=session_info.get('session_type', 'R')
        )
        
        # ä¿å­˜ç·©å­˜
        if severity_data:
            save_cache(severity_data, cache_key)
            if enable_debug:
                print("ğŸ’¾ åˆ†æçµæœå·²ç·©å­˜")
        
        if severity_data:
            return {
                "success": True,
                "message": "æˆåŠŸåŸ·è¡Œ åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ",
                "data": {
                    "function_id": 7,
                    "function_name": "Severity Distribution Analysis",
                    "analysis_type": "severity_distribution",
                    "session_info": session_info,
                    "severity_analysis": severity_data
                },
                "cache_used": cache_used,
                "cache_key": cache_key,
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "success": False,
                "message": "åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æåŸ·è¡Œå¤±æ•— - ç„¡å¯ç”¨æ•¸æ“š",
                "data": None,
                "timestamp": datetime.now().isoformat()
            }
            
    except Exception as e:
        if enable_debug:
            print(f"[ERROR] åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†ææ¨¡çµ„åŸ·è¡ŒéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
        return {
            "success": False,
            "message": f"åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æåŸ·è¡ŒéŒ¯èª¤: {str(e)}",
            "data": None,
            "timestamp": datetime.now().isoformat()
        }


def _display_cached_detailed_output(cached_data, session_info):
    """é¡¯ç¤ºç·©å­˜æ•¸æ“šçš„è©³ç´°è¼¸å‡º - Function 15 æ¨™æº–
    
    Args:
        cached_data: åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†ææ•¸æ“š
        session_info: è³½äº‹åŸºæœ¬ä¿¡æ¯
    """
    print("\nğŸ“Š è©³ç´°åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†æ (ç·©å­˜æ•¸æ“š)")
    print("=" * 80)
    
    if not cached_data:
        print("âŒ ç·©å­˜æ•¸æ“šç‚ºç©º")
        return
    
    print(f"ğŸ† è³½äº‹: {session_info.get('year', 'Unknown')} {session_info.get('event_name', 'Unknown')}")
    print(f"ğŸ è³½æ®µ: {session_info.get('session_type', 'Unknown')}")
    print(f"ğŸŸï¸ è³½é“: {session_info.get('circuit_name', 'Unknown')}")
    
    # ä½¿ç”¨åŸæœ‰çš„é¡¯ç¤ºå‡½æ•¸
    display_severity_distribution(cached_data)
    
    print("\nğŸ’¾ æ•¸æ“šä¾†æº: ç·©å­˜æª”æ¡ˆ")
    print("âœ… ç·©å­˜æ•¸æ“šè©³ç´°è¼¸å‡ºå®Œæˆ")


if __name__ == "__main__":
    print("F1 åš´é‡ç¨‹åº¦åˆ†ä½ˆåˆ†ææ¨¡çµ„ - ç¨ç«‹æ¸¬è©¦æ¨¡å¼")
    print("æ­¤æ¨¡çµ„éœ€è¦é…åˆ F1 æ•¸æ“šè¼‰å…¥å™¨ä½¿ç”¨")
